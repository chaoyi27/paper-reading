{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def archive_paper_under_date(filename, title, authors, tags, abstract, link, website_link, affiliation=None, notes=None, teaser_image_path=None, pipeline_image_path=None):\n",
    "    \"\"\"\n",
    "    Archives a paper record in a Markdown file under the current date's first-level header.\n",
    "    If the header for today's date does not exist, it's created.\n",
    "    \"\"\"\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    header = f\"# {current_date}\\n\\n\"\n",
    "    paper_record = generate_paper_record(title, authors, affiliation, tags, abstract, link, website_link, notes, teaser_image_path, pipeline_image_path)\n",
    "    \n",
    "    try:\n",
    "        with open(filename, \"r+\", encoding=\"utf-8\") as file:\n",
    "            content = file.readlines()\n",
    "            file.seek(0)\n",
    "            if content and content[0].strip() == header.strip():\n",
    "                # If the first header is today's date, archive under this header\n",
    "                content.insert(1, paper_record + \"\\n\")\n",
    "            else:\n",
    "                # Otherwise, prepend today's header and the record\n",
    "                content = [header] + [paper_record + \"\\n\"] + content\n",
    "            file.writelines(content)\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, create it with the header and the record\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(header + paper_record + \"\\n\")\n",
    "\n",
    "def generate_paper_record(\n",
    "    title, authors, affiliation, tags, abstract, link, website_link, notes, \n",
    "    teaser_image_path, pipeline_image_path):\n",
    "    \"\"\"\n",
    "    Generates the Markdown text for a paper record.\n",
    "    \"\"\"\n",
    "    record = f\"## {title}\\n\\n\"\n",
    "    # record += f\"- **Authors**: {', '.join(authors)}\\n\"\n",
    "    # authors is a string, not a list\n",
    "    record += f\"- **Authors**: {authors}\\n\"\n",
    "    if affiliation:\n",
    "        record += f\"- **Institutions**: {', '.join(affiliation)}\\n\"\n",
    "    tags_formatted = ', '.join([f\"`{tag}`\" for tag in tags])\n",
    "    record += f\"- **Tags**: {tags_formatted}\\n\\n\" \n",
    "    \n",
    "    record += f\"### Abstract\\n\\n{abstract}\\n\\n\"\n",
    "    record += f\"[Paper Link]({link})\\n\\n\"\n",
    "    \n",
    "    if teaser_image_path:\n",
    "        record += f\"![Teaser Image]({teaser_image_path})\\n\\n\"\n",
    "    if pipeline_image_path:\n",
    "        record += f\"![Pipeline Image]({pipeline_image_path})\\n\\n\"   \n",
    "    \n",
    "    if website_link != \"Project website not found\":\n",
    "        record += f\"[Website Link]({website_link})\\n\\n\"\n",
    "    \n",
    "    # if the notes is not a none, add a notes section\n",
    "    if notes:\n",
    "        record += f\"### Notes\\n\\n{notes}\\n\\n\"\n",
    "    \n",
    "    return record\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_paper_details(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract the paper title\n",
    "        title_element = soup.find('h1', class_='title mathjax')\n",
    "        title = title_element.text.replace('\\n', '').strip() if title_element else 'Title not found'\n",
    "        # move the \"Title\"\n",
    "        title = title.replace(\"Title:\", \"\").strip()\n",
    "\n",
    "        # Extract the author information\n",
    "        authors_element = soup.find('div', class_='authors')\n",
    "        authors = authors_element.text.replace('\\n', ' ').replace('Authors:', '').strip() if authors_element else 'Authors not found'\n",
    "\n",
    "        # Extract the abstract\n",
    "        abstract_element = soup.find('blockquote', class_='abstract mathjax')\n",
    "        abstract = abstract_element.text.replace('\\n', ' ').replace('Abstract:  ', '').strip() if abstract_element else 'Abstract not found'\n",
    "        # move the \"Abstract\"\n",
    "        abstract = abstract.replace(\"Abstract:\", \"\").strip()\n",
    "\n",
    "        # Extract the project website URL\n",
    "        project_website_element = soup.find('a', href=lambda href: href and \"leg-manip\" in href)\n",
    "        project_website_url = project_website_element['href'] if project_website_element else \"Project website not found\"\n",
    "\n",
    "        return title, authors, abstract, project_website_url\n",
    "        # return {\n",
    "        #     'Title': title,\n",
    "        #     'Authors': authors,\n",
    "        #     'Abstract': abstract,\n",
    "        #     'Project Website URL': project_website_url\n",
    "        # }\n",
    "    except Exception as e:\n",
    "        return {'Error': f'Failed to fetch details due to {e}'}\n",
    "\n",
    "# # Example usage\n",
    "# url = 'https://arxiv.org/abs/2403.20328'\n",
    "# paper_details = fetch_paper_details(url)\n",
    "# for key, value in paper_details.items():\n",
    "#     print(f'{key}: {value}')\n",
    "\n",
    "# url = 'https://arxiv.org/abs/2403.20328'\n",
    "# title, authors, abstract, project_website_url = fetch_paper_details(url)\n",
    "# print(title)\n",
    "# print(authors)\n",
    "# print(abstract)\n",
    "# print(project_website_url)\n",
    "\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = SimpleNamespace(\n",
    "    nav=\"Navigation\",\n",
    "    mm=\"Mobile Manipulation\",\n",
    "    s2r=\"Simulation to Reality\",\n",
    "    il=\"Imitation Learning\",\n",
    "    bc=\"Behavioral Cloning\",\n",
    "    rl=\"Reinforcement Learning\",\n",
    "    review=\"Review\",\n",
    "    llm=\"Large Language Models\",\n",
    "    nerf=\"NeRF\",\n",
    "    m=\"Manipulation\",\n",
    ")\n",
    "unis = SimpleNamespace(\n",
    "    MIT=\"Massachusetts Institute of Technology\",\n",
    "    Stanford=\"Stanford University\",\n",
    "    CMU=\"Carnegie Mellon University\",\n",
    "    UCB=\"University of California, Berkeley\",\n",
    "    Harvard=\"Harvard University\",\n",
    "    Oxford=\"University of Oxford\",\n",
    "    Cambridge=\"University of Cambridge\",\n",
    "    ETH=\"ETH Zurich - Swiss Federal Institute of Technology\",\n",
    "    Imperial=\"Imperial College London\",\n",
    "    Tsinghua=\"Tsinghua University\",\n",
    "    iiis=\"IIIS, Tsinghua University\",\n",
    "    PKU=\"Peking University\",\n",
    "    TUM=\"Technical University of Munich\",\n",
    "    HKUST=\"Hong Kong University of Science and Technology\",\n",
    "    CUHK=\"Chinese University of Hong Kong\",\n",
    ")\n",
    "ins = SimpleNamespace(\n",
    "    DeepMind=\"Google DeepMind\",\n",
    "    OpenAI=\"OpenAI\",\n",
    "    FAIR=\"Facebook AI Research\",\n",
    "    MSR=\"Microsoft Research\",\n",
    "    IBM=\"IBM Research\",\n",
    "    NVIDIA=\"NVIDIA Research\",\n",
    "    ShanghaiQizhi=\"Shanghai Qizhi Institute\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "link=\"https://arxiv.org/abs/2404.01812\"\n",
    "\n",
    "affiliation=[unis.CUHK, unis.TUM]\n",
    "t=[tags.nerf, tags.m]\n",
    "\n",
    "notes=\"These are the notes.\"\n",
    "teaser_image_path=\"imgs/2024-04-04_00-49.png\"\n",
    "pipeline_image_path=\"imgs/2024-04-04_00-30_1.png\"\n",
    "\n",
    "title, authors, abstract, project_website_url = fetch_paper_details(link)\n",
    "archive_paper_under_date(\n",
    "    filename=\"papers.md\",\n",
    "    title=title,authors=authors,tags=t,abstract=abstract,link=link,website_link=project_website_url,\n",
    "    \n",
    "    # affiliation=affiliation,\n",
    "    # notes=notes,\n",
    "    teaser_image_path=teaser_image_path,\n",
    "    # pipeline_image_path=pipeline_image_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "link=\"https://arxiv.org/abs/2403.20328\"\n",
    "\n",
    "affiliation=[ins.ShanghaiQizhi, unis.HKUST, unis.CMU, unis.iiis]\n",
    "t=[tags.rl, tags.bc, tags.mm]\n",
    "\n",
    "notes=\"These are the notes.\"\n",
    "teaser_image_path=\"../imgs/2024-04-04_00-30.png\"\n",
    "pipeline_image_path=\"../imgs/2024-04-04_00-30_1.png\"\n",
    "\n",
    "\n",
    "link=\"https://arxiv.org/abs/2403.19916\"\n",
    "\n",
    "affiliation=[unis.CUHK, unis.TUM]\n",
    "t=[tags.review, tags.il]\n",
    "\n",
    "notes=\"These are the notes.\"\n",
    "teaser_image_path=\"../imgs/2024-04-04_00-30.png\"\n",
    "pipeline_image_path=\"../imgs/2024-04-04_00-30_1.png\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
